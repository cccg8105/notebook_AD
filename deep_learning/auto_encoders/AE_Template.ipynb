{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE_Template.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOuzkLRxRx/gX7TgjyibUqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cccg8105/notebook_AD/blob/deep_learning/deep_learning/auto_encoders/AE_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIgX89iFe9l"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euXA6IRAFm9f"
      },
      "source": [
        "## Instalación de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8so-sM--FcWD",
        "outputId": "5657f36b-d009-4498-d67e-fe177bb649d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch===1.6.0 in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torchvision===0.7.0 in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch===1.6.0) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch===1.6.0) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision===0.7.0) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoLPybqPF23x"
      },
      "source": [
        "## Importar conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aHfdy6AF3qN"
      },
      "source": [
        "# Importar las librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "movies = pd.read_csv(\"https://raw.githubusercontent.com/cccg8105/deeplearning-az/master/datasets/Part%205%20-%20Boltzmann%20Machines%20(BM)/ml-1m/movies.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users  = pd.read_csv(\"https://raw.githubusercontent.com/cccg8105/deeplearning-az/master/datasets/Part%205%20-%20Boltzmann%20Machines%20(BM)/ml-1m/users.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings  = pd.read_csv(\"https://raw.githubusercontent.com/cccg8105/deeplearning-az/master/datasets/Part%205%20-%20Boltzmann%20Machines%20(BM)/ml-1m/ratings.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "training_set = pd.read_csv(\"https://raw.githubusercontent.com/cccg8105/deeplearning-az/master/datasets/Part%205%20-%20Boltzmann%20Machines%20(BM)/ml-100k/u1.base\", sep = \"\\t\", header = None)\n",
        "# se convierte la estructura para el manejo en pytorch\n",
        "training_set = np.array(training_set, dtype = \"int\")\n",
        "test_set = pd.read_csv(\"https://raw.githubusercontent.com/cccg8105/deeplearning-az/master/datasets/Part%205%20-%20Boltzmann%20Machines%20(BM)/ml-100k/u1.test\", sep = \"\\t\", header = None)\n",
        "# se convierte la estructura para el manejo en pytorch\n",
        "test_set = np.array(test_set, dtype = \"int\")\n",
        "\n",
        "\n",
        "nb_users = int(max(max(training_set[:, 0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7VEB0WjGAoR"
      },
      "source": [
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_user in range(1, nb_users+1):\n",
        "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
        "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies-1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6FsT-0JGpJf"
      },
      "source": [
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "# se convierten los datos a tipo de variable pytorch\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e9MjLsGp98"
      },
      "source": [
        "## Crear la arquitectura de la Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No5GvUpWGy7w"
      },
      "source": [
        "class SAE(nn.Module):\n",
        "    # constructor\n",
        "    def __init__(self, ):\n",
        "        nodosCapaOculta1 = 20\n",
        "        nodosCapaOculta2 = 10\n",
        "        nodosCapaOculta3 = 20\n",
        "\n",
        "        super(SAE, self).__init__()\n",
        "        # Creación de la primera capa con 20 nodos ocultos\n",
        "        self.fc1 = nn.Linear(nb_movies, nodosCapaOculta1)\n",
        "        # Creación de la segunda capa con 10 nodos ocultos\n",
        "        self.fc2 = nn.Linear(nodosCapaOculta1, nodosCapaOculta2)\n",
        "        # Creación de la tercera capa con 20 nodos ocultos\n",
        "        self.fc3 = nn.Linear(nodosCapaOculta2, nodosCapaOculta3)\n",
        "        # Creación de la capa final\n",
        "        self.fc4 = nn.Linear(nodosCapaOculta3, nb_movies)\n",
        "        # funcion de activacion\n",
        "        self.activation = nn.Sigmoid()\n",
        "    \n",
        "    #\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXOwnHJKMSF7"
      },
      "source": [
        "sae = SAE()\n",
        "# funcion de perdidas\n",
        "criterion = nn.MSELoss()\n",
        "# optimizador\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi_iKQgzN89w"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvXXnW1lN_Eb",
        "outputId": "40c316b0-bed8-4631-e8a6-18e60d9c4db7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# cantidad de epocas\n",
        "nb_epoch = 200\n",
        "\n",
        "for epoch in range(1, nb_epoch+1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        # entrada de la red neuronal \n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            # se obtiene prediccion de la red nueronal\n",
        "            output = sae.forward(input)\n",
        "            # se evita el calculo del gradiente descendente\n",
        "            target.require_grad = False\n",
        "            # se filtran las peliculas no valoradas\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10) \n",
        "            # se propaga error hacia atras\n",
        "            loss.backward()\n",
        "\n",
        "            train_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n",
        "            # conteo de usuarios\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print(\"Epoch: \"+str(epoch)+\", Loss: \"+str(train_loss/s))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: tensor(1.7711)\n",
            "Epoch: 2, Loss: tensor(1.0965)\n",
            "Epoch: 3, Loss: tensor(1.0533)\n",
            "Epoch: 4, Loss: tensor(1.0383)\n",
            "Epoch: 5, Loss: tensor(1.0307)\n",
            "Epoch: 6, Loss: tensor(1.0269)\n",
            "Epoch: 7, Loss: tensor(1.0238)\n",
            "Epoch: 8, Loss: tensor(1.0220)\n",
            "Epoch: 9, Loss: tensor(1.0208)\n",
            "Epoch: 10, Loss: tensor(1.0195)\n",
            "Epoch: 11, Loss: tensor(1.0187)\n",
            "Epoch: 12, Loss: tensor(1.0185)\n",
            "Epoch: 13, Loss: tensor(1.0177)\n",
            "Epoch: 14, Loss: tensor(1.0178)\n",
            "Epoch: 15, Loss: tensor(1.0172)\n",
            "Epoch: 16, Loss: tensor(1.0168)\n",
            "Epoch: 17, Loss: tensor(1.0167)\n",
            "Epoch: 18, Loss: tensor(1.0165)\n",
            "Epoch: 19, Loss: tensor(1.0163)\n",
            "Epoch: 20, Loss: tensor(1.0161)\n",
            "Epoch: 21, Loss: tensor(1.0160)\n",
            "Epoch: 22, Loss: tensor(1.0160)\n",
            "Epoch: 23, Loss: tensor(1.0157)\n",
            "Epoch: 24, Loss: tensor(1.0160)\n",
            "Epoch: 25, Loss: tensor(1.0155)\n",
            "Epoch: 26, Loss: tensor(1.0155)\n",
            "Epoch: 27, Loss: tensor(1.0154)\n",
            "Epoch: 28, Loss: tensor(1.0153)\n",
            "Epoch: 29, Loss: tensor(1.0130)\n",
            "Epoch: 30, Loss: tensor(1.0115)\n",
            "Epoch: 31, Loss: tensor(1.0106)\n",
            "Epoch: 32, Loss: tensor(1.0082)\n",
            "Epoch: 33, Loss: tensor(1.0083)\n",
            "Epoch: 34, Loss: tensor(1.0037)\n",
            "Epoch: 35, Loss: tensor(1.0038)\n",
            "Epoch: 36, Loss: tensor(1.0003)\n",
            "Epoch: 37, Loss: tensor(0.9967)\n",
            "Epoch: 38, Loss: tensor(0.9954)\n",
            "Epoch: 39, Loss: tensor(0.9954)\n",
            "Epoch: 40, Loss: tensor(0.9916)\n",
            "Epoch: 41, Loss: tensor(0.9909)\n",
            "Epoch: 42, Loss: tensor(0.9896)\n",
            "Epoch: 43, Loss: tensor(0.9874)\n",
            "Epoch: 44, Loss: tensor(0.9872)\n",
            "Epoch: 45, Loss: tensor(0.9832)\n",
            "Epoch: 46, Loss: tensor(0.9838)\n",
            "Epoch: 47, Loss: tensor(0.9821)\n",
            "Epoch: 48, Loss: tensor(0.9805)\n",
            "Epoch: 49, Loss: tensor(0.9822)\n",
            "Epoch: 50, Loss: tensor(0.9787)\n",
            "Epoch: 51, Loss: tensor(0.9763)\n",
            "Epoch: 52, Loss: tensor(0.9741)\n",
            "Epoch: 53, Loss: tensor(0.9744)\n",
            "Epoch: 54, Loss: tensor(0.9693)\n",
            "Epoch: 55, Loss: tensor(0.9690)\n",
            "Epoch: 56, Loss: tensor(0.9644)\n",
            "Epoch: 57, Loss: tensor(0.9662)\n",
            "Epoch: 58, Loss: tensor(0.9651)\n",
            "Epoch: 59, Loss: tensor(0.9631)\n",
            "Epoch: 60, Loss: tensor(0.9613)\n",
            "Epoch: 61, Loss: tensor(0.9601)\n",
            "Epoch: 62, Loss: tensor(0.9633)\n",
            "Epoch: 63, Loss: tensor(0.9594)\n",
            "Epoch: 64, Loss: tensor(0.9571)\n",
            "Epoch: 65, Loss: tensor(0.9537)\n",
            "Epoch: 66, Loss: tensor(0.9504)\n",
            "Epoch: 67, Loss: tensor(0.9527)\n",
            "Epoch: 68, Loss: tensor(0.9528)\n",
            "Epoch: 69, Loss: tensor(0.9567)\n",
            "Epoch: 70, Loss: tensor(0.9568)\n",
            "Epoch: 71, Loss: tensor(0.9520)\n",
            "Epoch: 72, Loss: tensor(0.9511)\n",
            "Epoch: 73, Loss: tensor(0.9515)\n",
            "Epoch: 74, Loss: tensor(0.9500)\n",
            "Epoch: 75, Loss: tensor(0.9476)\n",
            "Epoch: 76, Loss: tensor(0.9482)\n",
            "Epoch: 77, Loss: tensor(0.9494)\n",
            "Epoch: 78, Loss: tensor(0.9471)\n",
            "Epoch: 79, Loss: tensor(0.9463)\n",
            "Epoch: 80, Loss: tensor(0.9454)\n",
            "Epoch: 81, Loss: tensor(0.9448)\n",
            "Epoch: 82, Loss: tensor(0.9445)\n",
            "Epoch: 83, Loss: tensor(0.9442)\n",
            "Epoch: 84, Loss: tensor(0.9429)\n",
            "Epoch: 85, Loss: tensor(0.9425)\n",
            "Epoch: 86, Loss: tensor(0.9421)\n",
            "Epoch: 87, Loss: tensor(0.9410)\n",
            "Epoch: 88, Loss: tensor(0.9398)\n",
            "Epoch: 89, Loss: tensor(0.9406)\n",
            "Epoch: 90, Loss: tensor(0.9424)\n",
            "Epoch: 91, Loss: tensor(0.9425)\n",
            "Epoch: 92, Loss: tensor(0.9390)\n",
            "Epoch: 93, Loss: tensor(0.9408)\n",
            "Epoch: 94, Loss: tensor(0.9384)\n",
            "Epoch: 95, Loss: tensor(0.9394)\n",
            "Epoch: 96, Loss: tensor(0.9364)\n",
            "Epoch: 97, Loss: tensor(0.9375)\n",
            "Epoch: 98, Loss: tensor(0.9366)\n",
            "Epoch: 99, Loss: tensor(0.9378)\n",
            "Epoch: 100, Loss: tensor(0.9356)\n",
            "Epoch: 101, Loss: tensor(0.9359)\n",
            "Epoch: 102, Loss: tensor(0.9339)\n",
            "Epoch: 103, Loss: tensor(0.9344)\n",
            "Epoch: 104, Loss: tensor(0.9338)\n",
            "Epoch: 105, Loss: tensor(0.9341)\n",
            "Epoch: 106, Loss: tensor(0.9331)\n",
            "Epoch: 107, Loss: tensor(0.9329)\n",
            "Epoch: 108, Loss: tensor(0.9323)\n",
            "Epoch: 109, Loss: tensor(0.9331)\n",
            "Epoch: 110, Loss: tensor(0.9318)\n",
            "Epoch: 111, Loss: tensor(0.9322)\n",
            "Epoch: 112, Loss: tensor(0.9305)\n",
            "Epoch: 113, Loss: tensor(0.9309)\n",
            "Epoch: 114, Loss: tensor(0.9300)\n",
            "Epoch: 115, Loss: tensor(0.9306)\n",
            "Epoch: 116, Loss: tensor(0.9297)\n",
            "Epoch: 117, Loss: tensor(0.9300)\n",
            "Epoch: 118, Loss: tensor(0.9290)\n",
            "Epoch: 119, Loss: tensor(0.9298)\n",
            "Epoch: 120, Loss: tensor(0.9284)\n",
            "Epoch: 121, Loss: tensor(0.9292)\n",
            "Epoch: 122, Loss: tensor(0.9272)\n",
            "Epoch: 123, Loss: tensor(0.9292)\n",
            "Epoch: 124, Loss: tensor(0.9278)\n",
            "Epoch: 125, Loss: tensor(0.9278)\n",
            "Epoch: 126, Loss: tensor(0.9264)\n",
            "Epoch: 127, Loss: tensor(0.9273)\n",
            "Epoch: 128, Loss: tensor(0.9259)\n",
            "Epoch: 129, Loss: tensor(0.9268)\n",
            "Epoch: 130, Loss: tensor(0.9248)\n",
            "Epoch: 131, Loss: tensor(0.9261)\n",
            "Epoch: 132, Loss: tensor(0.9244)\n",
            "Epoch: 133, Loss: tensor(0.9251)\n",
            "Epoch: 134, Loss: tensor(0.9243)\n",
            "Epoch: 135, Loss: tensor(0.9246)\n",
            "Epoch: 136, Loss: tensor(0.9236)\n",
            "Epoch: 137, Loss: tensor(0.9243)\n",
            "Epoch: 138, Loss: tensor(0.9230)\n",
            "Epoch: 139, Loss: tensor(0.9236)\n",
            "Epoch: 140, Loss: tensor(0.9227)\n",
            "Epoch: 141, Loss: tensor(0.9231)\n",
            "Epoch: 142, Loss: tensor(0.9233)\n",
            "Epoch: 143, Loss: tensor(0.9263)\n",
            "Epoch: 144, Loss: tensor(0.9223)\n",
            "Epoch: 145, Loss: tensor(0.9220)\n",
            "Epoch: 146, Loss: tensor(0.9209)\n",
            "Epoch: 147, Loss: tensor(0.9215)\n",
            "Epoch: 148, Loss: tensor(0.9205)\n",
            "Epoch: 149, Loss: tensor(0.9216)\n",
            "Epoch: 150, Loss: tensor(0.9202)\n",
            "Epoch: 151, Loss: tensor(0.9208)\n",
            "Epoch: 152, Loss: tensor(0.9199)\n",
            "Epoch: 153, Loss: tensor(0.9207)\n",
            "Epoch: 154, Loss: tensor(0.9192)\n",
            "Epoch: 155, Loss: tensor(0.9200)\n",
            "Epoch: 156, Loss: tensor(0.9190)\n",
            "Epoch: 157, Loss: tensor(0.9201)\n",
            "Epoch: 158, Loss: tensor(0.9188)\n",
            "Epoch: 159, Loss: tensor(0.9196)\n",
            "Epoch: 160, Loss: tensor(0.9179)\n",
            "Epoch: 161, Loss: tensor(0.9191)\n",
            "Epoch: 162, Loss: tensor(0.9180)\n",
            "Epoch: 163, Loss: tensor(0.9188)\n",
            "Epoch: 164, Loss: tensor(0.9176)\n",
            "Epoch: 165, Loss: tensor(0.9182)\n",
            "Epoch: 166, Loss: tensor(0.9176)\n",
            "Epoch: 167, Loss: tensor(0.9178)\n",
            "Epoch: 168, Loss: tensor(0.9169)\n",
            "Epoch: 169, Loss: tensor(0.9171)\n",
            "Epoch: 170, Loss: tensor(0.9165)\n",
            "Epoch: 171, Loss: tensor(0.9171)\n",
            "Epoch: 172, Loss: tensor(0.9167)\n",
            "Epoch: 173, Loss: tensor(0.9166)\n",
            "Epoch: 174, Loss: tensor(0.9158)\n",
            "Epoch: 175, Loss: tensor(0.9164)\n",
            "Epoch: 176, Loss: tensor(0.9160)\n",
            "Epoch: 177, Loss: tensor(0.9168)\n",
            "Epoch: 178, Loss: tensor(0.9153)\n",
            "Epoch: 179, Loss: tensor(0.9157)\n",
            "Epoch: 180, Loss: tensor(0.9148)\n",
            "Epoch: 181, Loss: tensor(0.9161)\n",
            "Epoch: 182, Loss: tensor(0.9147)\n",
            "Epoch: 183, Loss: tensor(0.9155)\n",
            "Epoch: 184, Loss: tensor(0.9144)\n",
            "Epoch: 185, Loss: tensor(0.9152)\n",
            "Epoch: 186, Loss: tensor(0.9144)\n",
            "Epoch: 187, Loss: tensor(0.9147)\n",
            "Epoch: 188, Loss: tensor(0.9134)\n",
            "Epoch: 189, Loss: tensor(0.9146)\n",
            "Epoch: 190, Loss: tensor(0.9137)\n",
            "Epoch: 191, Loss: tensor(0.9181)\n",
            "Epoch: 192, Loss: tensor(0.9137)\n",
            "Epoch: 193, Loss: tensor(0.9139)\n",
            "Epoch: 194, Loss: tensor(0.9131)\n",
            "Epoch: 195, Loss: tensor(0.9143)\n",
            "Epoch: 196, Loss: tensor(0.9131)\n",
            "Epoch: 197, Loss: tensor(0.9136)\n",
            "Epoch: 198, Loss: tensor(0.9128)\n",
            "Epoch: 199, Loss: tensor(0.9131)\n",
            "Epoch: 200, Loss: tensor(0.9124)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPZuMzpEOX-q"
      },
      "source": [
        "## Evaluar conjunto test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuugCEU4ObYS",
        "outputId": "b83227d5-05be-4ae7-919e-8e258487cbc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    # entrada con todas las peliculas que ya vio el usuario\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    # objetivo de peliculas que desea estimar la calificacion\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    \n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae.forward(input)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10) \n",
        "        test_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n",
        "        s += 1.\n",
        "\n",
        "print(\"Test Loss: \"+str(test_loss/s))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: tensor(0.9553)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}